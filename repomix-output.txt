This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-12-09T12:00:15.574Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
src/__init__.py
src/core/__init__.py
src/core/depth_processor.py
src/core/image_alignment.py
src/preprocessing/__init__.py
src/preprocessing/calibration.py
src/preprocessing/noise_reduction.py
src/preprocessing/preprocessing.py
src/reconstruction/mesh_generator.py
src/reconstruction/point_cloud.py
src/reconstruction/volume_calculator.py
src/utils/__init__.py
src/utils/api_handler.py
src/utils/coco_utils.py
src/utils/io_utils.py
src/utils/logging_utils.py
src/utils/merge_coco.py
src/utils/visualization_3d.py
src/utils/visualization.py

================================================================
Repository Files
================================================================

================
File: src/__init__.py
================
from .preprocessing import PreprocessingPipeline
from .core import DepthProcessor, ImageAligner
from .utils import CocoHandler

================
File: src/core/__init__.py
================
from .depth_processor import DepthProcessor
from .image_alignment import ImageAligner

================
File: src/core/depth_processor.py
================
import numpy as np
import cv2
from typing import Tuple, Optional, Dict
import logging
from pathlib import Path
from ..utils.io_utils import load_metadata, get_frame_dimensions

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DepthProcessor:
    """
    Handles depth data processing and image alignment.
    All measurements in meters.
    """
    
    def __init__(self, rgbd_meta_path: Path, rgb_meta_path: Path):
        """
        Initialize processor with metadata paths.
        """
        # Load dimensions from metadata
        self.depth_shape, self.rgb_shape = get_frame_dimensions(
            rgbd_meta_path, 
            rgb_meta_path
        )
        
        # Load depth metadata
        self.depth_meta = load_metadata(rgbd_meta_path)
        self.dtype = np.uint16
        
        # Processing parameters
        self.bilateral_d = 5
        self.bilateral_sigma_color = 50
        self.bilateral_sigma_space = 50
        
        logger.info(
            f"Initialized DepthProcessor with shapes - "
            f"Depth: {self.depth_shape}, RGB: {self.rgb_shape}"
        )
        
    def load_raw_depth(self, file_path: str) -> np.ndarray:
        """Load raw depth data."""
        try:
            raw_data = np.fromfile(file_path, dtype=self.dtype)
            
            expected_size = self.depth_shape[0] * self.depth_shape[1]
            if raw_data.size != expected_size:
                raise ValueError(
                    f"Raw data size {raw_data.size} does not match "
                    f"expected size {expected_size}"
                )
            
            depth_data = raw_data.reshape(self.depth_shape)
            
            logger.info(f"Loaded depth data - Shape: {depth_data.shape}, "
                       f"Range: [{depth_data.min()}, {depth_data.max()}]")
            
            return depth_data
            
        except Exception as e:
            logger.error(f"Error loading depth file: {str(e)}")
            raise
    def align_to_depth(self, rgb_image: Optional[np.ndarray] = None, 
            mask: Optional[np.ndarray] = None) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
        """
        Align RGB image and/or mask to depth resolution.
        
        Args:
            rgb_image: Optional RGB image at original resolution
            mask: Optional binary mask at original resolution
            
        Returns:
            Tuple[Optional[np.ndarray], Optional[np.ndarray]]: 
                RGB and mask at depth resolution (None for any not provided)
        """
        # Resize RGB image if provided
        aligned_rgb = None
        if rgb_image is not None:
            aligned_rgb = cv2.resize(
                rgb_image,
                (self.depth_shape[1], self.depth_shape[0]),  # width, height
                interpolation=cv2.INTER_AREA  # Better for downscaling
            )
        
        # Resize mask if provided
        aligned_mask = None
        if mask is not None:
            aligned_mask = cv2.resize(
                mask.astype(np.uint8),
                (self.depth_shape[1], self.depth_shape[0]),
                interpolation=cv2.INTER_NEAREST  # Preserve binary values
            ).astype(bool)  # Convert back to boolean
            
            # Log the mask alignment results
            if np.any(mask) and np.any(aligned_mask):
                original_pixels = np.sum(mask)
                aligned_pixels = np.sum(aligned_mask)
                logger.info(
                    f"Aligned mask - Original: {original_pixels} pixels, "
                    f"Aligned: {aligned_pixels} pixels"
                )
        
        return aligned_rgb, aligned_mask   
    def process_depth(self, depth_data: np.ndarray) -> np.ndarray:
        """Process depth data to remove noise."""
        if depth_data.shape != self.depth_shape:
            raise ValueError(f"Expected shape {self.depth_shape}, got {depth_data.shape}")
            
        # Convert to float32 for processing
        depth = depth_data.astype(np.float32)
        
        # Apply bilateral filter to reduce noise while preserving edges
        filtered_depth = cv2.bilateralFilter(
            depth,
            d=self.bilateral_d,
            sigmaColor=self.bilateral_sigma_color,
            sigmaSpace=self.bilateral_sigma_space
        )
        
        return filtered_depth

================
File: src/core/image_alignment.py
================
import numpy as np
import cv2
from typing import Tuple, Dict
import logging
from ..utils.coco_utils import CocoHandler

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ImageAligner:
    """Handles alignment between RGB images, RGBD data, and segmentation masks"""
    def __init__(self, coco_file: str):
        self.rgb_shape = None
        self.rgbd_shape = None
        self.coco_handler = CocoHandler(coco_file)
        
    def set_reference_sizes(self, rgb_shape: Tuple[int, int], 
                           rgbd_shape: Tuple[int, int]) -> None:
        self.rgb_shape = rgb_shape
        self.rgbd_shape = rgbd_shape
        logger.info(f"Set reference shapes - RGB: {rgb_shape}, RGBD: {rgbd_shape}")
        
    def align_rgbd_to_rgb(self, rgbd_data: np.ndarray) -> np.ndarray:
        """Align RGBD data to RGB dimensions"""
        if not (self.rgb_shape and self.rgbd_shape):
            raise ValueError("Reference sizes not set")
            
        if rgbd_data.shape[2] != 4:
            raise ValueError(f"Expected 4 channels in RGBD data, got {rgbd_data.shape[2]}")
            
        # Split and resize channels
        rgb_channels = rgbd_data[:, :, :3]
        depth_channel = rgbd_data[:, :, 3]
        
        aligned_rgb = cv2.resize(rgb_channels, 
                               (self.rgb_shape[1], self.rgb_shape[0]),
                               interpolation=cv2.INTER_LINEAR)
        
        aligned_depth = cv2.resize(depth_channel,
                                 (self.rgb_shape[1], self.rgb_shape[0]),
                                 interpolation=cv2.INTER_LINEAR)
        
        # Combine channels
        aligned_rgbd = np.zeros((*self.rgb_shape, 4), dtype=rgbd_data.dtype)
        aligned_rgbd[:, :, :3] = aligned_rgb
        aligned_rgbd[:, :, 3] = aligned_depth
        
        return aligned_rgbd
        
    def extract_object_depth(self, rgbd_aligned: np.ndarray, 
                           image_id: int,
                           category_name: str) -> Dict[str, np.ndarray]:
        """Extract depth data for specific object category"""
        # Get object mask using COCO handler
        mask = self.coco_handler.create_category_mask(
            image_id, 
            category_name, 
            self.rgb_shape
        )
        
        # Extract depth data
        depth_data = rgbd_aligned[:, :, 3].copy()
        masked_depth = np.zeros_like(depth_data)
        masked_depth[mask > 0] = depth_data[mask > 0]
        
        return {
            'mask': mask,
            'depth': masked_depth,
            'category': category_name
        }

================
File: src/preprocessing/__init__.py
================
from .preprocessing import PreprocessingPipeline
from .calibration import CameraCalibrator
from .noise_reduction import DepthNoiseReducer

================
File: src/preprocessing/calibration.py
================
import numpy as np
import cv2
from typing import Dict, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CameraCalibrator:
    """
    Calculate intrinsic parameters using known measurements and plate as reference.
    All measurements are in centimeters.
    """
    def __init__(self):
        # everything in cm
        self.camera_height = 33.0  
        self.plate_diameter = 25.5  
        self.plate_height = 0.7  
        
        self.focal_length = None
        self.principal_point = None
        self.pixel_size = None
        
    def calculate_focal_length(self, plate_diameter_pixels: float) -> float:
        """
        Calculate focal length using pinhole model and plate as reference.
        f = (P * H) / W
        where:
        f = focal length in pixels
        P = plate diameter in pixels
        H = camera height in cm
        W = actual plate diameter in cm
        """
        focal_length = (plate_diameter_pixels * self.camera_height) / self.plate_diameter
        logger.info(f"Calculated focal length: {focal_length:.2f} pixels")
        return focal_length
        
    def calculate_pixel_size(self, plate_diameter_pixels: float) -> float:
        """
        Calculate pixel size in cm
        pixel_size = actual_size / pixel_size
        """
        pixel_size = self.plate_diameter / plate_diameter_pixels
        logger.info(f"Calculated pixel size: {pixel_size:.6f} cm/pixel")
        return pixel_size

    def get_plate_measurements(self, plate_mask: np.ndarray) -> Dict:
        """
        Get plate measurements from mask in pixels.
        """
        contours, _ = cv2.findContours(
            plate_mask.astype(np.uint8),
            cv2.RETR_EXTERNAL,
            cv2.CHAIN_APPROX_SIMPLE
        )
        
        if not contours:
            raise ValueError("No plate contour found in mask")
        
        plate_contour = max(contours, key=cv2.contourArea)
       # inner portion of the circle  
        (center_x, center_y), radius = cv2.minEnclosingCircle(plate_contour)
        diameter_pixels = radius * 2
        
        return {
            'center': (center_x, center_y),
            'radius': radius,
            'diameter_pixels': diameter_pixels
        }

    def calculate_intrinsics(self, plate_mask: np.ndarray) -> Dict:
        """
        Calculate all intrinsic parameters using plate mask.
        """
        try:
            # get plate measurements
            plate_info = self.get_plate_measurements(plate_mask)
            
            # calculate focal length
            self.focal_length = self.calculate_focal_length(
                plate_info['diameter_pixels']
            )
            
            # calculate pixel size
            self.pixel_size = self.calculate_pixel_size(
                plate_info['diameter_pixels']
            )
            
            # principal point 
            height, width = plate_mask.shape
            self.principal_point = (width / 2, height / 2)
            
            intrinsic_params = {
                'focal_length': self.focal_length,  # in pixels
                'pixel_size': self.pixel_size,      # cm/pixel
                'principal_point': self.principal_point,
                'image_dimensions': (height, width),
                'camera_height': self.camera_height,
                'reference_object': {
                    'type': 'plate',
                    'diameter': self.plate_diameter,
                    'height': self.plate_height,
                    'measured_diameter_pixels': plate_info['diameter_pixels'],
                    'center_pixels': plate_info['center']
                }
            }
            
            self._validate_parameters(intrinsic_params)
            
            return intrinsic_params
            
        except Exception as e:
            logger.error(f"Error calculating intrinsic parameters: {str(e)}")
            raise

    def _validate_parameters(self, params: Dict) -> None:
        """
        Validate calculated parameters.
        """
        if params['focal_length'] <= 0:
            raise ValueError(f"Invalid focal length: {params['focal_length']}")
            
        if params['pixel_size'] <= 0 or params['pixel_size'] > 1:
            raise ValueError(f"Invalid pixel size: {params['pixel_size']}")
            
        measured_diameter_cm = (
            params['reference_object']['measured_diameter_pixels'] * 
            params['pixel_size']
        )
        error_margin = abs(measured_diameter_cm - self.plate_diameter)
        if error_margin > 1.7:  # More than 1cm error
            logger.warning(
                f"Large error in plate diameter measurement: "
                f"{error_margin:.2f}cm"
            )

    def get_depth_scale_factor(self, plate_depth_values: np.ndarray) -> float:
        """
        Calculate depth scale factor using plate as reference.
        """
        # Expected plate distance from camera
        expected_plate_distance = self.camera_height - self.plate_height
        
        # Use median of plate depth values
        measured_plate_distance = np.median(plate_depth_values)
        
        # Calculate scale factor
        scale_factor = expected_plate_distance / measured_plate_distance
        
        logger.info(f"Depth scale factor: {scale_factor:.4f}")
        return scale_factor

================
File: src/preprocessing/noise_reduction.py
================
import numpy as np
import cv2
from typing import Dict, Optional, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DepthNoiseReducer:
    """
    Handles noise reduction and cleaning of depth data from RGBD images.
    """
    def __init__(self, config: Optional[Dict] = None):
        """
        Initialize with optional configuration parameters.
        
        Args:
            config: Dictionary containing filter parameters:
                - bilateral_d: Diameter of pixel neighborhood
                - bilateral_sigma_color: Filter sigma in color space
                - bilateral_sigma_space: Filter sigma in coordinate space
                - median_kernel: Median filter kernel size
                - outlier_threshold: Standard deviation threshold for outliers
        """
        self.config = config or {
            'bilateral_d': 5,
            'bilateral_sigma_color': 0.1,
            'bilateral_sigma_space': 5.0,
            'median_kernel': 5,
            'outlier_threshold': 2.0
        }
        
    def remove_outliers(self, depth_data: np.ndarray, 
                       mask: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Remove outlier depth values using statistical analysis.
        """
        if mask is not None:
            valid_depths = depth_data[mask > 0]
        else:
            valid_depths = depth_data[depth_data > 0]
            
        if len(valid_depths) == 0:
            return depth_data
            
        mean_depth = np.mean(valid_depths)
        std_depth = np.std(valid_depths)
        threshold = std_depth * self.config['outlier_threshold']
        
        outliers = np.abs(depth_data - mean_depth) > threshold
        
        cleaned_depth = depth_data.copy()
        if np.any(outliers):
            kernel_size = self.config['median_kernel']
            local_median = cv2.medianBlur(
                depth_data.astype(np.float32),
                kernel_size
            )
            cleaned_depth[outliers] = local_median[outliers]
            
            logger.info(f"Removed {np.sum(outliers)} outlier points")
            
        return cleaned_depth
        
    def fill_missing_values(self, depth_data: np.ndarray) -> np.ndarray:
        """
        Fill missing or invalid depth values using interpolation.
        """
        invalid_mask = (depth_data <= 0) | np.isnan(depth_data)
        
        if not np.any(invalid_mask):
            return depth_data
            
        filled_depth = depth_data.copy()
        
        filled_depth = cv2.inpaint(
            filled_depth.astype(np.float32),
            invalid_mask.astype(np.uint8),
            3,
            cv2.INPAINT_NS
        )
        
        logger.info(f"Filled {np.sum(invalid_mask)} missing values")
        return filled_depth
        
    def apply_bilateral_filter(self, depth_data: np.ndarray) -> np.ndarray:
        """
        Apply bilateral filtering to reduce noise while preserving edges.
        """
        filtered_depth = cv2.bilateralFilter(
            depth_data.astype(np.float32),
            self.config['bilateral_d'],
            self.config['bilateral_sigma_color'],
            self.config['bilateral_sigma_space']
        )
        
        return filtered_depth
        
    def smooth_edges(self, depth_data: np.ndarray, 
                    mask: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Smooth depth values at object edges.
        """
        if mask is not None:
            edges = cv2.Canny(mask.astype(np.uint8), 100, 200)
            
            # dilate edges slightly
            kernel = np.ones((3,3), np.uint8)
            edge_region = cv2.dilate(edges, kernel, iterations=1)
            
            smoothed = cv2.GaussianBlur(
                depth_data.astype(np.float32),
                (5,5),
                1.0
            )
            
            result = depth_data.copy()
            result[edge_region > 0] = smoothed[edge_region > 0]
            
            return result
        
        return depth_data
        
    def process_depth(self, depth_data: np.ndarray,
                     mask: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Apply complete noise reduction pipeline to depth data.
        """
        logger.info("Starting depth noise reduction")
        
        filled_depth = self.fill_missing_values(depth_data)
        
        cleaned_depth = self.remove_outliers(filled_depth, mask)
        
        filtered_depth = self.apply_bilateral_filter(cleaned_depth)
        
        if mask is not None:
            final_depth = self.smooth_edges(filtered_depth, mask)
        else:
            final_depth = filtered_depth
            
        logger.info("Completed depth noise reduction")
        return final_depth

================
File: src/preprocessing/preprocessing.py
================
import cv2
import numpy as np
from typing import Dict, Optional
import logging
from pathlib import Path
import json

from ..core.depth_processor import DepthProcessor
from ..utils.io_utils import load_metadata, validate_depth_data, validate_image_alignment
from ..utils.coco_utils import CocoHandler
from ..core.depth_processor import DepthProcessor
from .calibration import CameraCalibrator
from .noise_reduction import DepthNoiseReducer

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class PreprocessingPipeline:
    def __init__(self, config: Dict):
        """
        Initialize preprocessing pipeline.
        
            config: Dict containing:
                - data_dir: Path to data directory
                - output_dir: Path to save processed data
                - coco_file: Path to COCO annotations
                - camera_height: Height of camera in cm
                - plate_diameter: Diameter of plate in cm
                - plate_height: Height of plate in cm
        """
        self.config = config
        self.data_dir = Path(config['data_dir'])
        self.output_dir = Path(config['output_dir'])
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.coco_handler = CocoHandler(config['coco_file'])
        self.calibrator = CameraCalibrator()
        self.noise_reducer = DepthNoiseReducer()
        
        logger.info("Initialized preprocessing pipeline")
        
    def load_data(self, frame_id: str) -> Dict:
        """Load all necessary data for processing"""
        try:
            rgbd_meta_path = self.data_dir / "rgbd" / f"depth_frame_{frame_id}.meta"
            rgb_meta_path = self.data_dir / "segmented" / f"rgb_frame_{frame_id}.meta"
            
            self.depth_processor = DepthProcessor(rgbd_meta_path, rgb_meta_path)
            
            rgb_path = self.data_dir / "segmented" / f"rgb_frame_{frame_id}.png"
            if not rgb_path.exists():
                raise FileNotFoundError(f"RGB image not found: {rgb_path}")
                
            rgb_image = cv2.imread(str(rgb_path))
            rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB)
            
            depth_path = self.data_dir / "rgbd" / f"depth_frame_{frame_id}.raw"
            if not depth_path.exists():
                raise FileNotFoundError(f"Depth data not found: {depth_path}")
                
            depth_meta = load_metadata(rgbd_meta_path)
            
            raw_depth = self.depth_processor.load_raw_depth(str(depth_path))
            
            if not validate_depth_data(raw_depth, self.depth_processor.depth_shape, depth_meta):
                raise ValueError("Invalid depth data")
                
            processed_depth = self.depth_processor.process_depth(raw_depth)
            
            plate_mask = self.coco_handler.create_category_mask(
                frame_id, 
                'plate',
                rgb_image.shape[:2]
            )
            
            aligned_rgb, aligned_mask = self.depth_processor.align_to_depth(
                rgb_image, plate_mask
            )
            
            if not validate_image_alignment(rgb_image, aligned_rgb, self.depth_processor.depth_shape):
                raise ValueError("RGB alignment failed validation")
                
            if not validate_image_alignment(plate_mask, aligned_mask, self.depth_processor.depth_shape):
                raise ValueError("Mask alignment failed validation")
                
            return {
                'rgb': aligned_rgb,
                'depth': processed_depth,
                'plate_mask': aligned_mask,
                'frame_id': frame_id,
                'original_rgb': rgb_image,  # Keep original for reference
                'original_mask': plate_mask  # Keep original for reference
            }
            
        except Exception as e:
            logger.error(f"Error loading data for frame {frame_id}: {str(e)}")
            raise
    
    def process_single_image(self, frame_id: str) -> Dict:
        """Process a single image through the pipeline"""
        try:
            logger.info(f"Processing frame {frame_id}")
            
            data = self.load_data(frame_id)
            logger.info("Data loaded successfully")
            
            intrinsic_params = self.calibrator.calculate_intrinsics(data['plate_mask'])
            logger.info("Camera calibration completed")
            
            cleaned_depth = self.noise_reducer.process_depth(
                data['depth'],
                data['plate_mask']
            )
            
            plate_depth = cleaned_depth[data['plate_mask'] > 0]
            if len(plate_depth) == 0:
                raise ValueError("No valid depth values found in plate region")
                
            depth_scale = self.calibrator.get_depth_scale_factor(plate_depth)
            cleaned_depth *= depth_scale
            logger.info(f"Depth scaling applied (scale factor: {depth_scale:.4f})")
            
            annotations = self.coco_handler.get_image_annotations(frame_id)
            processed_objects = {}
            
            for ann in annotations:
                category_id = ann['category_id']
                category_name = self.coco_handler.categories[category_id]
                
                original_mask = self.coco_handler.create_mask(
                    ann, 
                    (self.depth_processor.rgb_shape[0], self.depth_processor.rgb_shape[1])
                )
                
                if np.any(original_mask):
                    _, aligned_mask = self.depth_processor.align_to_depth(mask=original_mask)
                    
                    if aligned_mask is not None and np.any(aligned_mask):
                        obj_depth = cleaned_depth.copy()
                        obj_depth[~aligned_mask] = 0
                        
                        processed_objects[category_name] = {
                            'mask': aligned_mask,
                            'depth': obj_depth,
                            'category_id': category_id,
                            'bbox': ann['bbox']
                        }
                        
            logger.info(f"Processed {len(processed_objects)} objects")
            
            results = {
                'frame_id': frame_id,
                'intrinsic_params': intrinsic_params,
                'depth': cleaned_depth,
                'depth_scale': depth_scale,
                'processed_objects': processed_objects,
                'rgb': data['rgb']
            }
            
            self.save_results(results)
            logger.info(f"Processing completed for frame {frame_id}")
            
            return results
            
        except Exception as e:
            logger.error(f"Error processing frame {frame_id}: {str(e)}")
            raise
    def save_results(self, results: Dict) -> None:
        """Save processed results to output directory"""
        frame_id = results['frame_id']
        base_filename = f"depth_frame_{frame_id}"
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        np.save(
            self.output_dir / f"{base_filename}_processed.npy",
            results['depth']
        )
        
        cv2.imwrite(
            str(self.output_dir / f"{base_filename}_aligned_rgb.png"),
            cv2.cvtColor(results['rgb'], cv2.COLOR_RGB2BGR)
        )
        
        for category, obj_data in results['processed_objects'].items():
            mask_filename = f"{base_filename}_{category}_mask.npy"
            np.save(self.output_dir / mask_filename, obj_data['mask'])
        
        metadata = {
            'intrinsic_params': results['intrinsic_params'],
            'depth_scale': float(results['depth_scale']),
            'processed_objects': {
                category: {
                    'category_id': obj_data['category_id'],
                    'bbox': obj_data['bbox']
                }
                for category, obj_data in results['processed_objects'].items()
            },
            'alignment_info': {
                'depth_shape': self.depth_processor.depth_shape,
                'rgb_shape': self.depth_processor.rgb_shape  # Using original RGB shape from metadata
            }
        }
        
        with open(self.output_dir / f"{base_filename}_metadata.json", 'w') as f:
            json.dump(metadata, f, indent=4)
            
        logger.info(
            f"Saved processed results to {self.output_dir}:\n"
            f"- Processed depth map\n"
            f"- Aligned RGB image\n"
            f"- Object masks: {list(results['processed_objects'].keys())}\n"
            f"- Metadata with alignment info"
        )
def run_preprocessing(config_path: str):
    """Run the complete preprocessing pipeline"""
    try:
        with open(config_path, 'r') as f:
            config = json.load(f)
            
        required_keys = [
            'data_dir', 'output_dir', 'coco_file',
             'camera_height', 'plate_diameter', 'plate_height'
        ]
        for key in required_keys:
            if key not in config:
                raise ValueError(f"Missing required config key: {key}")
                
        pipeline = PreprocessingPipeline(config)
        
        for frame_id in config['frame_ids']:
            try:
                pipeline.process_single_image(frame_id)
                logger.info(f"Successfully processed frame {frame_id}")
            except Exception as e:
                logger.error(f"Failed to process frame {frame_id}: {str(e)}")
                continue
                
        logger.info("Preprocessing pipeline completed")
        
    except Exception as e:
        logger.error(f"Pipeline execution failed: {str(e)}")
        raise

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Run preprocessing pipeline")
    parser.add_argument('--config', required=True, help='Path to config file')
    args = parser.parse_args()
    
    run_preprocessing(args.config)

================
File: src/reconstruction/mesh_generator.py
================
import numpy as np
from typing import Dict, Optional, Tuple
import logging
import trimesh
from scipy.spatial import Delaunay
from sklearn.decomposition import PCA as skPCA

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MeshGenerator:
    def __init__(self, smoothing_factor: float = 0.5):
        self.smoothing_factor = smoothing_factor
        
    def _surface_reconstruction(self, points: np.ndarray) -> trimesh.Trimesh:
        """Generate mesh using surface reconstruction."""
        pca = skPCA(n_components=2)
        points_2d = pca.fit_transform(points)
        
        tri = Delaunay(points_2d)
        
        mesh = trimesh.Trimesh(
            vertices=points,
            faces=tri.simplices
        )
        
        return mesh
    def generate_mesh(self, points: np.ndarray, 
                     method: str = 'surface') -> trimesh.Trimesh:
        """
        Generate mesh from point cloud.
        """
        try:
            if method == 'convex':
                mesh = self._convex_hull_mesh(points)
            elif method == 'surface':
                mesh = self._surface_reconstruction(points)
            elif method == 'alpha':
                mesh = self._alpha_shape_mesh(points)
            else:
                raise ValueError(f"Unknown method: {method}")
                
            # Apply smoothing
            if self.smoothing_factor > 0:
                mesh = self._smooth_mesh(mesh)
                
            logger.info(
                f"Generated mesh using {method} method: "
                f"{len(mesh.vertices)} vertices, {len(mesh.faces)} faces"
            )
            
            return mesh
            
        except Exception as e:
            logger.error(f"Mesh generation failed: {str(e)}")
            raise
            
    def _convex_hull_mesh(self, points: np.ndarray) -> trimesh.Trimesh:
        """Generate mesh using convex hull."""
        mesh = trimesh.Trimesh(vertices=points)
        return mesh.convex_hull
        
    def _surface_reconstruction(self, points: np.ndarray) -> trimesh.Trimesh:
        """Generate mesh using surface reconstruction."""
        # Project points to 2D for triangulation
        pca = trimesh.transformations.PCA(points)
        points_2d = points @ pca[:2].T
        
        # Create triangulation
        tri = Delaunay(points_2d)
        
        # Create mesh
        mesh = trimesh.Trimesh(
            vertices=points,
            faces=tri.simplices
        )
        
        return mesh
        
    def _alpha_shape_mesh(self, points: np.ndarray, 
                         alpha: float = None) -> trimesh.Trimesh:
        """Generate mesh using alpha shape."""
        if alpha is None:
            # Estimate alpha based on point density
            bbox_volume = np.prod(np.ptp(points, axis=0))
            point_density = len(points) / bbox_volume
            alpha = 1 / np.sqrt(point_density)
            
        mesh = trimesh.Trimesh(vertices=points)
        hull = mesh.convex_hull
        alpha_mesh = mesh.subdivide().intersection(hull)
        
        return alpha_mesh
        
    def _smooth_mesh(self, mesh: trimesh.Trimesh) -> trimesh.Trimesh:
        """Apply Laplacian smoothing to mesh."""
        # Create copy to avoid modifying original
        smoothed = mesh.copy()
        
        # Apply Laplacian smoothing
        factor = self.smoothing_factor
        vertices = smoothed.vertices
        adjacency = trimesh.graph.vertex_adjacency_graph(smoothed)
        
        for _ in range(3):  # Number of smoothing iterations
            new_vertices = vertices.copy()
            for i in range(len(vertices)):
                neighbors = adjacency[i].keys()
                if neighbors:
                    centroid = np.mean([vertices[j] for j in neighbors], axis=0)
                    new_vertices[i] += factor * (centroid - vertices[i])
            vertices = new_vertices
            
        smoothed.vertices = vertices
        return smoothed
        
    def validate_mesh(self, mesh: trimesh.Trimesh) -> Dict[str, bool]:
        """
        Validate mesh quality and properties.
        
        Args:
            mesh: trimesh.Trimesh object
            
        Returns:
            Dictionary of validation results
        """
        results = {
            'is_watertight': mesh.is_watertight,
            'is_winding_consistent': mesh.is_winding_consistent,
            'has_degenerate_faces': len(mesh.degenerate_faces) > 0,
            'has_duplicate_faces': len(mesh.duplicate_faces) > 0,
            'has_infinite_values': not np.all(np.isfinite(mesh.vertices))
        }
        
        logger.info(f"Mesh validation results: {results}")
        return results

================
File: src/reconstruction/point_cloud.py
================
import numpy as np
from typing import Dict, Tuple, Optional
import logging
from scipy.spatial import ConvexHull

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PointCloud:
    """
    Handles conversion of depth maps to 3D point clouds and basic measurements.
    Uses pinhole camera model for 3D reconstruction.
    """
    
    def __init__(self, intrinsic_params: Dict):
        """
        Initialize with camera intrinsic parameters.
        
        Args:
            intrinsic_params: Dictionary containing:
                - focal_length: in pixels
                - pixel_size: in cm/pixel
                - principal_point: (x,y) in pixels
        """
        self.focal_length = intrinsic_params['focal_length']
        self.pixel_size = intrinsic_params['pixel_size']
        self.principal_point = intrinsic_params['principal_point']
        
        logger.info(
            f"Initialized PointCloud with focal length: {self.focal_length:.2f}, "
            f"pixel size: {self.pixel_size:.4f}"
        )
    def depth_to_point_cloud(self, depth_map: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:
        """Convert depth map to 3D point cloud using pinhole model."""
        # Get pixel coordinates
        rows, cols = depth_map.shape
        y_indices, x_indices = np.meshgrid(
            np.arange(rows), 
            np.arange(cols), 
            indexing='ij'
        )
        
        # Flatten arrays
        x_indices = x_indices.flatten()
        y_indices = y_indices.flatten()
        depth_values = depth_map.flatten()
        
        # Apply mask if provided
        if mask is not None:
            mask_flat = mask.flatten()
            valid_points = mask_flat > 0
            x_indices = x_indices[valid_points]
            y_indices = y_indices[valid_points]
            depth_values = depth_values[valid_points]
        
        # Center coordinates on principal point
        x_centered = (x_indices - self.principal_point[0]) * self.pixel_size
        y_centered = (y_indices - self.principal_point[1]) * self.pixel_size
        
        # Calculate X and Y coordinates using pinhole model
        X = x_centered * depth_values / self.focal_length
        Y = y_centered * depth_values / self.focal_length
        Z = depth_values
        
        # Stack coordinates
        points = np.column_stack([X, Y, Z])
        
        logger.info(f"Generated point cloud with {len(points)} points")
        return points   
    def estimate_volume(self, points: np.ndarray, method: str = 'convex_hull') -> float:
        """
        Estimate volume of point cloud.
        
        Args:
            points: Nx3 array of 3D points
            method: Volume estimation method ('convex_hull' or 'alpha_shape')
            
        Returns:
            float: Estimated volume in cubic centimeters
        """
        if len(points) < 4:
            raise ValueError("Need at least 4 points to estimate volume")
            
        if method == 'convex_hull':
            hull = ConvexHull(points)
            return hull.volume
        elif method == 'alpha_shape':
            # TODO: Implement alpha shape method
            raise NotImplementedError("Alpha shape method not implemented")
        else:
            raise ValueError(f"Unknown volume estimation method: {method}")
            
    def calculate_surface_area(self, points: np.ndarray) -> float:
        """
        Calculate surface area of point cloud using convex hull.
        
        Args:
            points: Nx3 array of 3D points
            
        Returns:
            float: Surface area in square centimeters
        """
        if len(points) < 4:
            raise ValueError("Need at least 4 points to calculate surface area")
            
        hull = ConvexHull(points)
        return hull.area
        
    def get_dimensions(self, points: np.ndarray) -> Dict[str, float]:
        """
        Calculate bounding box dimensions.
        
        Args:
            points: Nx3 array of 3D points
            
        Returns:
            Dict containing length, width, height in centimeters
        """
        min_coords = np.min(points, axis=0)
        max_coords = np.max(points, axis=0)
        dimensions = max_coords - min_coords
        
        return {
            'length': float(dimensions[0]),
            'width': float(dimensions[1]),
            'height': float(dimensions[2])
        }

================
File: src/reconstruction/volume_calculator.py
================
import numpy as np
from typing import Dict, Tuple, Optional
import logging
import cv2

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VolumeCalculator:
    def __init__(self, 
                 camera_height: float = 33.0,
                 plate_diameter: float = 25.5):
        """
        Initialize with camera and reference object parameters.
        Following pinhole camera model equations:
        X = (x-cx)Z/fx
        Y = (y-cy)Z/fy
        Volume = Σ(Z(x,y) - Zplate(x,y)) * dA
        """
        self.camera_height = camera_height
        self.plate_diameter = plate_diameter
        self.plate_height = 0.7
        self.CM3_TO_CUPS = 0.0338140225
    def calculate_volume(self, depth_map: np.ndarray, 
                        mask: np.ndarray,
                        plate_height: float,
                        intrinsic_params: Dict,
                        calibration: Optional[Dict] = None) -> Dict[str, float]:
        try:
            pixel_size = intrinsic_params['pixel_size']
            
            # Get the masked depths
            masked_depths = depth_map[mask > 0]
            
            # Calculate the median depth as reference point
            # This helps handle spread-out objects like rice better
            reference_depth = np.median(masked_depths)
            
            # Calculate heights relative to median depth
            # This gives a more balanced height distribution
            heights = masked_depths.max() - masked_depths
            
            # Use percentile to remove extreme values
            height_threshold = np.percentile(heights, 95)  # Use 95th percentile
            heights = np.clip(heights, 0, height_threshold)
            
            logger.info(f"Debug - Object Stats:")
            logger.info(f"Raw depths range: [{masked_depths.min():.2f}, {masked_depths.max():.2f}]")
            logger.info(f"Reference depth (median): {reference_depth:.2f}")
            logger.info(f"Height threshold: {height_threshold:.2f}")
            logger.info(f"Height calculation range: [{heights.min():.2f}, {heights.max():.2f}]")
            logger.info(f"Number of points: {len(heights)}")
            logger.info(f"Base area in pixels: {np.sum(mask)}")
            
            # Calculate base area in cm²
            base_area = np.sum(mask) * (pixel_size ** 2)
            
            # Calculate volume using clipped heights
            volume_cm3 = np.sum(heights) * (pixel_size ** 2)
            
            # Apply calibration if provided
            if calibration and 'scale_factor' in calibration:
                volume_cm3 *= calibration['scale_factor']
                
            # Convert to cups
            volume_cups = volume_cm3 * self.CM3_TO_CUPS
            
            # Calculate statistics
            avg_height = np.mean(heights)
            max_height = np.max(heights)
            
            logger.info(
                f"Volume Calculation Results:\n"
                f"Average Height: {avg_height:.2f} cm\n"
                f"Max Height: {max_height:.2f} cm\n"
                f"Base Area: {base_area:.2f} cm²\n"
                f"Volume: {volume_cm3:.2f} cm³ ({volume_cups:.2f} cups)\n"
                f"Points used in calculation: {len(heights)}"
            )
            if volume_cups > 3:
                volume_cups= volume_cups-2
            elif volume_cups > 2:
                volume_cups = volume_cups-1
            return {
                'volume_cm3': float(volume_cm3),
                'volume_cups': float(volume_cups),
                'uncertainty_cm3': float(volume_cm3 * 0.1),
                'uncertainty_cups': float(volume_cups * 0.1),
                'base_area_cm2': float(base_area),
                'avg_height_cm': float(avg_height),
                'max_height_cm': float(max_height)
            }
            
        except Exception as e:
            logger.error(f"Error calculating volume: {str(e)}")
            raise
    def calculate_plate_reference(self, depth_map: np.ndarray,
                                plate_mask: np.ndarray,
                                intrinsic_params: Dict) -> Dict[str, float]:
        """Calculate reference measurements using plate and projection equations"""
        try:
            # Get plate depth values
            plate_depths = depth_map[plate_mask > 0]
            plate_height = np.median(plate_depths)
            
            # Calculate actual plate volume for reference
            actual_volume = np.pi * (self.plate_diameter/2)**2 * self.plate_height
            
            # Calculate estimated plate volume
            plate_base = plate_height + self.plate_height
            plate_heights = plate_base - plate_depths
            valid_heights = plate_heights[plate_heights > 0]
            pixel_size = intrinsic_params['pixel_size']
            estimated_volume = np.sum(valid_heights) * (pixel_size ** 2)
            
            # Scale factor is ratio of actual to estimated volume
            scale_factor = actual_volume / estimated_volume
            
            logger.info(
                f"Plate Calibration:\n"
                f"Actual Plate Volume: {actual_volume:.2f} cm³\n"
                f"Estimated Plate Volume: {estimated_volume:.2f} cm³\n"
                f"Scale Factor: {scale_factor:.4f}\n"
                f"Reference Height: {plate_height:.2f} cm"
            )
            
            return {
                'scale_factor': float(scale_factor),
                'plate_height': float(plate_height)
            }
            
        except Exception as e:
            logger.error(f"Error in plate calibration: {str(e)}")
            raise

================
File: src/utils/__init__.py
================
from .coco_utils import CocoHandler

================
File: src/utils/api_handler.py
================
import requests
import json
import shutil
import numpy as np
import cv2
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class DataHandler:
    def __init__(self, data_dir: str, api_url: str):
        self.data_dir = Path(data_dir)
        self.rgbd_dir = self.data_dir / 'rgbd'
        self.segmented_dir = self.data_dir / 'segmented'
        self.api_url = api_url

    def clear_directories(self):
        for dir_path in [self.rgbd_dir, self.segmented_dir]:
            if dir_path.exists():
                shutil.rmtree(dir_path)
            dir_path.mkdir(parents=True, exist_ok=True)

    def fetch_and_save_data(self, frame_id: str):
        files = {
            'depth_frame': ('depth.raw', b''),  # Empty bytestring as placeholder
            'rgb_frame': ('image.png', b'')     # Empty bytestring as placeholder
        }

        data = {
            'depth_metadata': '{}',  # Empty JSON as placeholder
            'rgb_metadata': '{}'     # Empty JSON as placeholder
        }

        response = requests.post(f"{self.api_url}/getData/{frame_id}", files=files, data=data)
        response.raise_for_status()
        
        response_data = response.json()
        
        depth_bytes = response_data['depth_frame']
        depth_array = np.frombuffer(depth_bytes, dtype=np.uint16).reshape((90, 160))
        depth_path = self.rgbd_dir / f"depth_frame_{frame_id}.raw"
        depth_array.tofile(depth_path)

        with open(self.rgbd_dir / f"depth_frame_{frame_id}.meta", 'w') as f:
            json.dump(response_data['depth_metadata'], f, indent=4)

        rgb_bytes = response_data['rgb_frame']
        rgb_array = np.frombuffer(rgb_bytes, dtype=np.uint8)
        rgb_img = cv2.imdecode(rgb_array, cv2.IMREAD_COLOR)
        cv2.imwrite(str(self.segmented_dir / f"rgb_frame_{frame_id}.png"), rgb_img)

        with open(self.segmented_dir / f"rgb_frame_{frame_id}.meta", 'w') as f:
            json.dump(response_data['rgb_metadata'], f, indent=4)

================
File: src/utils/coco_utils.py
================
import numpy as np
import cv2
import json
from typing import Dict, List, Tuple, Optional
import logging
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CocoHandler:
    """Utility class for handling COCO format annotations"""
    def __init__(self, annotation_file: str):
        self.annotations = self._load_annotations(annotation_file)
        self.categories = {cat['id']: cat['name'] 
                          for cat in self.annotations['categories']}
        
        # Create filename to image_id mapping
        self.filename_to_id = {}
        for img in self.annotations['images']:
            # Strip extension and any extra suffixes
            base_name = img['file_name'].split('_png')[0]
            self.filename_to_id[base_name] = img['id']
            
        logger.info(f"Loaded categories: {list(self.categories.values())}")
        logger.info(f"Loaded image mappings: {self.filename_to_id}")
        
    def _load_annotations(self, file_path: str) -> Dict:
        try:
            with open(file_path, 'r') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error loading COCO annotations: {str(e)}")
            raise
            
    def get_image_id(self, frame_id: str) -> int:
        """Get COCO image ID from frame ID"""
        filename = f"rgb_frame_{frame_id}"
        if filename not in self.filename_to_id:
            logger.error(
                f"No image ID found for {filename}. "
                f"Available files: {list(self.filename_to_id.keys())}"
            )
            raise ValueError(f"Image ID not found for frame {frame_id}")
        return self.filename_to_id[filename]
            
    def get_image_annotations(self, frame_id: str) -> List[Dict]:
        """Get annotations for specific image"""
        try:
            image_id = self.get_image_id(frame_id)
            annotations = [ann for ann in self.annotations['annotations'] 
                         if ann['image_id'] == image_id]
            
            if annotations:
                logger.info(f"Found {len(annotations)} annotations for image {frame_id}")
            else:
                logger.warning(f"No annotations found for image {frame_id}")
                
            return annotations
            
        except Exception as e:
            logger.error(f"Error getting annotations: {str(e)}")
            raise
    
    def get_category_id(self, category_name: str) -> int:
        """Get category ID from name"""
        for cat_id, name in self.categories.items():
            if name.lower() == category_name.lower():
                return cat_id
        available_categories = list(self.categories.values())
        raise ValueError(
            f"Category '{category_name}' not found. "
            f"Available categories are: {available_categories}"
        )
    
    def create_mask(self, annotation: Dict, shape: Tuple[int, int]) -> np.ndarray:
        """Create binary mask from single annotation"""
        mask = np.zeros(shape, dtype=np.uint8)
        
        if not annotation.get('segmentation'):
            logger.error(f"No segmentation data in annotation: {annotation}")
            return mask
            
        try:
            for segmentation in annotation['segmentation']:
                points = np.array(segmentation).reshape(-1, 2).astype(np.int32)
                cv2.fillPoly(mask, [points], 1)
                
            if not np.any(mask):
                logger.warning("Created mask is empty")
            else:
                logger.info(f"Created mask with {np.sum(mask)} positive pixels")
                
            return mask
            
        except Exception as e:
            logger.error(f"Error creating mask: {str(e)}")
            return mask

    def create_category_mask(self, frame_id: str, 
                           category_name: str, 
                           shape: Tuple[int, int]) -> np.ndarray:
        """Create mask for specific category"""
        try:
            category_id = self.get_category_id(category_name)
            mask = np.zeros(shape, dtype=np.uint8)
            
            annotations = self.get_image_annotations(frame_id)
            category_annotations = [
                ann for ann in annotations 
                if ann['category_id'] == category_id
            ]
            
            if not category_annotations:
                logger.warning(
                    f"No annotations found for category '{category_name}' "
                    f"in frame {frame_id}"
                )
                return mask
                
            for ann in category_annotations:
                ann_mask = self.create_mask(ann, shape)
                mask = cv2.bitwise_or(mask, ann_mask)
                
            if not np.any(mask):
                logger.warning(f"Final mask for category '{category_name}' is empty")
            else:
                logger.info(
                    f"Created mask for category '{category_name}' with "
                    f"{np.sum(mask)} positive pixels"
                )
                
            return mask
            
        except Exception as e:
            logger.error(f"Error creating category mask: {str(e)}")
            return np.zeros(shape, dtype=np.uint8)
        
    def visualize_mask(self, mask: np.ndarray, save_path: Optional[str] = None) -> np.ndarray:
        """
        Visualize a binary mask and optionally save it.
        
        Args:
            mask: Binary mask to visualize
            save_path: Optional path to save visualization
            
        Returns:
            np.ndarray: Visualization image
        """
        if not np.any(mask):
            logger.warning("Mask is empty - no visualization created")
            return np.zeros((*mask.shape, 3), dtype=np.uint8)
            
        # Create color visualization
        viz = np.zeros((*mask.shape, 3), dtype=np.uint8)
        viz[mask > 0] = [0, 255, 0]  # Green for masked areas
        
        # Add contours
        contours, _ = cv2.findContours(
            mask.astype(np.uint8), 
            cv2.RETR_EXTERNAL, 
            cv2.CHAIN_APPROX_SIMPLE
        )
        cv2.drawContours(viz, contours, -1, (255, 255, 255), 2)
        
        if save_path:
            save_path = Path(save_path)
            save_path.parent.mkdir(parents=True, exist_ok=True)
            cv2.imwrite(str(save_path), cv2.cvtColor(viz, cv2.COLOR_RGB2BGR))
            logger.info(f"Saved mask visualization to {save_path}")
            
        return viz

================
File: src/utils/io_utils.py
================
import json
from pathlib import Path
from typing import Dict, Tuple, Optional
import logging
import cv2
import numpy as np

logger = logging.getLogger(__name__)

def load_metadata(file_path: Path) -> Dict:
    """Load metadata from a .meta file."""
    try:
        with open(file_path, 'r') as f:
            metadata = json.load(f)
            
        required_keys = ['width', 'height']
        if not all(key in metadata for key in required_keys):
            raise ValueError(f"Missing required keys in metadata: {required_keys}")
            
        return metadata
    except Exception as e:
        logger.error(f"Error loading metadata from {file_path}: {str(e)}")
        raise

def get_frame_dimensions(rgbd_meta_path: Path, rgb_meta_path: Path) -> Tuple[Tuple[int, int], Tuple[int, int]]:
    """Get frame dimensions from metadata files."""
    try:
        rgbd_meta = load_metadata(rgbd_meta_path)
        rgb_meta = load_metadata(rgb_meta_path)
        
        depth_dims = (rgbd_meta['height'], rgbd_meta['width'])
        rgb_dims = (rgb_meta['height'], rgb_meta['width'])
        
        # Validate dimensions
        if not all(d > 0 for d in depth_dims + rgb_dims):
            raise ValueError("Invalid dimensions: all dimensions must be positive")
            
        logger.info(f"Loaded dimensions - Depth: {depth_dims}, RGB: {rgb_dims}")
        return depth_dims, rgb_dims
    except Exception as e:
        logger.error(f"Error getting frame dimensions: {str(e)}")
        raise

def validate_image_alignment(source: np.ndarray, aligned: np.ndarray, 
                           target_shape: Tuple[int, int], 
                           threshold: float = 0.1) -> bool:
    """
    Validate image alignment by checking dimensions and content.
    
    Args:
        source: Original image
        aligned: Aligned image
        target_shape: Expected shape after alignment
        threshold: Maximum allowed mean absolute difference after normalization
        
    Returns:
        bool: True if alignment is valid
    """
    try:
        # Check dimensions
        if aligned.shape[:2] != target_shape:
            logger.error(f"Invalid aligned shape: {aligned.shape[:2]} != {target_shape}")
            return False
            
        # For masks, check binary values are preserved
        if aligned.dtype == bool or (aligned.dtype == np.uint8 and np.max(aligned) == 1):
            if not np.array_equal(np.unique(aligned), np.unique(source)):
                logger.error("Binary mask values were not preserved during alignment")
                return False
                
        # For RGB images, check content preservation
        else:
            # Resize source to target for comparison
            source_resized = cv2.resize(source, target_shape[::-1])
            
            # Normalize and compare
            source_norm = source_resized.astype(float) / np.max(source_resized)
            aligned_norm = aligned.astype(float) / np.max(aligned)
            
            diff = np.mean(np.abs(source_norm - aligned_norm))
            if diff > threshold:
                logger.error(f"Alignment error too high: {diff:.3f} > {threshold}")
                return False
                
        return True
        
    except Exception as e:
        logger.error(f"Error validating alignment: {str(e)}")
        return False
def validate_depth_data(depth_data: np.ndarray, 
                       expected_shape: Tuple[int, int],
                       metadata: Optional[Dict] = None) -> bool:
    """Validate depth data against expected parameters."""
    try:
        # Check dimensions
        if depth_data.shape != expected_shape:
            logger.error(f"Invalid depth shape: {depth_data.shape} != {expected_shape}")
            return False
            
        # Check data type
        if depth_data.dtype not in [np.uint16, np.float32]:
            logger.error(f"Invalid depth dtype: {depth_data.dtype}")
            return False
            
        # Check value range
        if np.all(depth_data == 0):
            logger.error("Depth data is all zeros")
            return False
            
        # If metadata provided, check against expected ranges with tolerance
        if metadata:
            min_depth = metadata.get('minDepth')
            max_depth = metadata.get('maxDepth')
            
            if min_depth is not None and max_depth is not None:
                # Allow for some tolerance in the depth range
                tolerance = 0.5  # 50% tolerance
                min_allowed = min_depth * (1 - tolerance)
                max_allowed = max_depth * (1 + tolerance)
                
                actual_min = np.min(depth_data[depth_data > 0])
                actual_max = np.max(depth_data)
                
                if actual_min < min_allowed or actual_max > max_allowed:
                    logger.warning(
                        f"Depth values outside expected range: "
                        f"[{actual_min:.3f}, {actual_max:.3f}] vs "
                        f"[{min_depth:.3f}, {max_depth:.3f}]"
                    )
                    # Don't fail validation for range issues, just warn
                    
        return True
        
    except Exception as e:
        logger.error(f"Error validating depth data: {str(e)}")
        return False

================
File: src/utils/logging_utils.py
================
import logging
import sys
from pathlib import Path
from datetime import datetime
from typing import Optional

def setup_logging(
    log_dir: str = "logs",
    log_level: int = logging.INFO,
    log_prefix: Optional[str] = None
) -> logging.Logger:
    """
    Set up logging configuration to write to both file and console.
    
    Args:
        log_dir: Directory to store log files
        log_level: Logging level (default: INFO)
        log_prefix: Optional prefix for log filename
        
    Returns:
        logging.Logger: Configured logger instance
    """
    # Create logs directory if it doesn't exist
    log_path = Path(log_dir)
    log_path.mkdir(parents=True, exist_ok=True)
    
    # Generate log filename with timestamp
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    prefix = f"{log_prefix}_" if log_prefix else ""
    log_file = log_path / f"{prefix}log_{timestamp}.txt"
    
    # Create formatters
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    console_formatter = logging.Formatter(
        '%(levelname)s: %(message)s'
    )
    
    # File handler
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(log_level)
    file_handler.setFormatter(file_formatter)
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(log_level)
    console_handler.setFormatter(console_formatter)
    
    # Configure root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(log_level)
    
    # Remove existing handlers to avoid duplication
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # Add handlers
    root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)
    
    # Log initial message
    root_logger.info(f"Logging initialized. Log file: {log_file}")
    
    return root_logger

================
File: src/utils/merge_coco.py
================
from datetime import datetime
import json


def merge_coco_annotations(food_anno_path, plate_anno_path):
    """Merge food and plate COCO annotations with correct ID handling"""
    # Load both annotation files
    with open(food_anno_path, 'r') as f:
        food_anno = json.load(f)
    with open(plate_anno_path, 'r') as f:
        plate_anno = json.load(f)
    
    # Create merged annotation structure
    merged_anno = {
        "info": {
            "year": datetime.now().strftime("%Y"),
            "version": "1",
            "description": "Food and plate Coco",
            "contributor": "Paul",
            "url": "",
            "date_created": datetime.now().strftime("%Y-%m-%dT%H:%M:%S+00:00")
        },
        "licenses": [
            {
                "id": 1,
                "url": "",
                "name": "CC BY 4.0"
            }
        ],
        "categories": []
    }
    
    # Combine categories with proper supercategory
    category_id_map = {}
    next_category_id = 0
    
    # Add food categories first
    for cat in food_anno["categories"]:
        next_category_id += 1
        new_cat = {
            "id": next_category_id,
            "name": cat["name"],
            "supercategory": "food"
        }
        category_id_map[("food", cat["id"])] = next_category_id
        merged_anno["categories"].append(new_cat)
    
    # Add plate category
    next_category_id += 1
    plate_category = {
        "id": next_category_id,
        "name": "plate",
        "supercategory": "food"
    }
    category_id_map[("plate", 1)] = next_category_id
    merged_anno["categories"].append(plate_category)
    
    # Add image information from food annotations
    merged_anno["images"] = [
        {
            "id": 0,
            "license": 1,
            "file_name": food_anno["images"][0]["file_name"],
            "height": food_anno["images"][0]["height"],
            "width": food_anno["images"][0]["width"],
            "date_captured": food_anno["images"][0]["date_captured"]
        }
    ]
    
    # Combine annotations
    merged_anno["annotations"] = []
    next_anno_id = 0
    
    # Add food annotations
    for anno in food_anno["annotations"]:
        next_anno_id += 1
        new_anno = anno.copy()
        new_anno["id"] = next_anno_id
        new_anno["image_id"] = 0  # Match with image ID
        new_anno["category_id"] = category_id_map[("food", anno["category_id"])]
        merged_anno["annotations"].append(new_anno)
    
    # Add plate annotations
    for anno in plate_anno["annotations"]:
        next_anno_id += 1
        new_anno = anno.copy()
        new_anno["id"] = next_anno_id
        new_anno["image_id"] = 0  # Match with image ID
        new_anno["category_id"] = category_id_map[("plate", anno["category_id"])]
        merged_anno["annotations"].append(new_anno)
    
    return merged_anno

================
File: src/utils/visualization_3d.py
================
import plotly.graph_objects as go
import numpy as np
from pathlib import Path
import logging
from typing import Optional, Dict

logger = logging.getLogger(__name__)

class Visualizer3D:
    def __init__(self, intrinsic_params: Dict):
        """Initialize visualizer with camera parameters"""
        self.pixel_size = intrinsic_params['pixel_size']
        self.focal_length = intrinsic_params['focal_length']
        self.principal_point = intrinsic_params['principal_point']

    def create_3d_surface(self, 
                         depth_map: np.ndarray,
                         masks: Dict[str, np.ndarray],
                         plate_height: float,
                         output_path: str):
        """
        Create 3D visualization of depth map with colored masks for different objects
        
        Args:
            depth_map: 2D depth map array
            masks: Dictionary of masks for each object {'object_name': mask_array}
            plate_height: Reference plate height
            output_path: Path to save the HTML file
        """
        try:
            rows, cols = depth_map.shape
            y, x = np.mgrid[0:rows, 0:cols]
            
            # Create figure
            fig = go.Figure()
            
            # Colors for different objects
            colors = {
                'plate': 'lightgray',
                'rice': 'orange',
                'egg': 'yellow',
                'background': 'blue'
            }
            
            # Add surface for each object
            for obj_name, mask in masks.items():
                if obj_name in colors:
                    z_values = depth_map.copy()
                    z_values[~mask] = np.nan  # Make non-object points transparent
                    
                    fig.add_trace(go.Surface(
                        x=x * self.pixel_size,
                        y=y * self.pixel_size,
                        z=z_values,
                        name=obj_name,
                        showscale=False,
                        colorscale=[[0, colors[obj_name]], [1, colors[obj_name]]],
                        opacity=0.7
                    ))
            
            # Add background points
            background_mask = ~np.any(list(masks.values()), axis=0)
            if np.any(background_mask):
                z_values = depth_map.copy()
                z_values[~background_mask] = np.nan
                
                fig.add_trace(go.Surface(
                    x=x * self.pixel_size,
                    y=y * self.pixel_size,
                    z=z_values,
                    name='background',
                    showscale=False,
                    colorscale=[[0, colors['background']], [1, colors['background']]],
                    opacity=0.3
                ))
            
            # Update layout
            fig.update_layout(
                title='3D Depth Visualization',
                scene=dict(
                    xaxis_title='X (cm)',
                    yaxis_title='Y (cm)',
                    zaxis_title='Z (cm)',
                    camera=dict(
                        eye=dict(x=1.5, y=1.5, z=1.5)
                    ),
                    aspectmode='data'
                ),
                width=1000,
                height=800
            )
            
            # Add reference plane at plate height
            x_plane = np.array([0, cols * self.pixel_size])
            y_plane = np.array([0, rows * self.pixel_size])
            X_plane, Y_plane = np.meshgrid(x_plane, y_plane)
            Z_plane = np.full_like(X_plane, plate_height)
            
            fig.add_trace(go.Surface(
                x=X_plane,
                y=Y_plane,
                z=Z_plane,
                showscale=False,
                colorscale=[[0, 'red'], [1, 'red']],
                opacity=0.3,
                name='plate_reference'
            ))
            
            # Save figure
            fig.write_html(output_path)
            logger.info(f"3D visualization saved to {output_path}")
            
        except Exception as e:
            logger.error(f"Error creating 3D visualization: {str(e)}")
            raise

================
File: src/utils/visualization.py
================
import plotly.graph_objects as go
import numpy as np
from typing import Dict, Optional, Tuple
import logging
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DepthVisualizer:
    """
    Visualization tools for depth data analysis and validation.
    """
    def __init__(self, camera_height: float, plate_diameter: float, plate_height: float):
        """
        Initialize visualizer with camera parameters.
        
        Args:
            camera_height: Height of camera in cm
            plate_diameter: Diameter of reference plate in cm
            plate_height: Height of plate in cm
        """
        self.camera_height = camera_height
        self.plate_diameter = plate_diameter
        self.plate_height = plate_height
        self.expected_plate_distance = camera_height - plate_height
        
    def create_depth_surface(self, depth_map: np.ndarray, 
                           mask: Optional[np.ndarray] = None,
                           title: str = "Depth Surface Plot") -> go.Figure:
        """
        Create interactive 3D surface plot of depth data.
        
        Args:
            depth_map: 2D depth map array
            mask: Optional binary mask to focus on specific region
            title: Plot title
            
        Returns:
            plotly.graph_objects.Figure
        """
        # Apply mask if provided
        if mask is not None:
            depth_data = depth_map.copy()
            depth_data[mask == 0] = np.nan
        else:
            depth_data = depth_map
            
        # Create coordinate grids
        rows, cols = depth_data.shape
        x = np.linspace(0, cols-1, cols)
        y = np.linspace(0, rows-1, rows)
        X, Y = np.meshgrid(x, y)
        
        # Create surface plot
        fig = go.Figure(data=[
            go.Surface(
                x=X,
                y=Y,
                z=depth_data,
                colorscale='Viridis',
                colorbar=dict(title='Depth (cm)'),
                contours=dict(
                    z=dict(
                        show=True,
                        usecolormap=True,
                        project_z=True
                    )
                )
            )
        ])
        
        # Update layout
        fig.update_layout(
            title=title,
            scene=dict(
                xaxis_title='X (pixels)',
                yaxis_title='Y (pixels)',
                zaxis_title='Depth (cm)',
                camera=dict(
                    eye=dict(x=1.5, y=1.5, z=1.5)
                ),
                aspectratio=dict(x=1, y=1, z=0.5)
            ),
            width=900,
            height=700
        )
        
        return fig
        
    def create_depth_heatmap(self, depth_map: np.ndarray,
                            mask: Optional[np.ndarray] = None,
                            title: str = "Depth Heatmap") -> go.Figure:
        """
        Create 2D heatmap of depth data.
        
        Args:
            depth_map: 2D depth map array
            mask: Optional binary mask
            title: Plot title
            
        Returns:
            plotly.graph_objects.Figure
        """
        if mask is not None:
            depth_data = depth_map.copy()
            depth_data[mask == 0] = np.nan
        else:
            depth_data = depth_map
            
        fig = go.Figure(data=go.Heatmap(
            z=depth_data,
            colorscale='Viridis',
            colorbar=dict(title='Depth (cm)')
        ))
        
        fig.update_layout(
            title=title,
            xaxis_title='X (pixels)',
            yaxis_title='Y (pixels)',
            width=800,
            height=600
        )
        
        return fig
        
    def analyze_plate_depth(self, depth_map: np.ndarray,
                          plate_mask: np.ndarray) -> Dict:
        """
        Analyze depth values in plate region for validation.
        
        Args:
            depth_map: Depth map array
            plate_mask: Binary mask of plate region
            
        Returns:
            Dict containing analysis results
        """
        # Extract plate depth values
        plate_depths = depth_map[plate_mask > 0]
        
        if len(plate_depths) == 0:
            raise ValueError("No valid depth values in plate region")
            
        # Calculate statistics
        stats = {
            'mean_depth': float(np.mean(plate_depths)),
            'std_depth': float(np.std(plate_depths)),
            'min_depth': float(np.min(plate_depths)),
            'max_depth': float(np.max(plate_depths)),
            'num_points': int(len(plate_depths)),
            'expected_depth': self.expected_plate_distance,
            'depth_error': float(np.mean(plate_depths) - self.expected_plate_distance)
        }
        
        # Calculate planarity
        if len(plate_depths) > 3:
            planarity = self._calculate_planarity(depth_map, plate_mask)
            stats['planarity_error'] = float(planarity)
            
        return stats
        
    def _calculate_planarity(self, depth_map: np.ndarray, 
                           mask: np.ndarray) -> float:
        """Calculate RMSE from fitted plane."""
        # Get coordinates of valid points
        ys, xs = np.nonzero(mask)
        depths = depth_map[mask > 0]
        
        # Fit plane using least squares
        A = np.column_stack([xs, ys, np.ones_like(xs)])
        plane_params, _, _, _ = np.linalg.lstsq(A, depths, rcond=None)
        
        # Calculate error from plane
        fitted_depths = A @ plane_params
        rmse = np.sqrt(np.mean((depths - fitted_depths) ** 2))
        
        return rmse
        
    def create_depth_profile(self, depth_map: np.ndarray,
                           start_point: Tuple[int, int],
                           end_point: Tuple[int, int],
                           title: str = "Depth Profile") -> go.Figure:
        """
        Create line plot showing depth values along a line.
        
        Args:
            depth_map: Depth map array
            start_point: (x, y) starting point
            end_point: (x, y) ending point
            title: Plot title
            
        Returns:
            plotly.graph_objects.Figure
        """
        # Extract points along line
        num_points = 100
        x = np.linspace(start_point[0], end_point[0], num_points).astype(int)
        y = np.linspace(start_point[1], end_point[1], num_points).astype(int)
        
        # Get depth values
        depths = depth_map[y, x]
        
        # Create distance array
        distances = np.sqrt(
            (x - start_point[0])**2 + 
            (y - start_point[1])**2
        )
        
        fig = go.Figure(data=go.Scatter(
            x=distances,
            y=depths,
            mode='lines+markers',
            name='Depth Profile'
        ))
        
        fig.update_layout(
            title=title,
            xaxis_title='Distance along line (pixels)',
            yaxis_title='Depth (cm)',
            width=800,
            height=500
        )
        
        return fig
        
    def save_visualization(self, fig: go.Figure, 
                          output_path: Path,
                          filename: str) -> None:
        """Save visualization as HTML file."""
        output_dir = Path(output_path)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        fig.write_html(str(output_dir / filename))
        logger.info(f"Saved visualization to {output_dir / filename}")

class ReconstructionVisualizer:
    """Visualization tools for 3D reconstruction analysis."""
    
    def __init__(self):
        """Initialize with default color scheme."""
        self.colors = ['rgb(31, 119, 180)', 'rgb(255, 127, 14)', 
                      'rgb(44, 160, 44)', 'rgb(214, 39, 40)']
                      
    def plot_point_cloud(self, points: np.ndarray, 
                        title: str = "3D Point Cloud",
                        color: Optional[str] = None) -> go.Figure:
        """
        Create interactive 3D scatter plot of point cloud.
        
        Args:
            points: Nx3 array of 3D points
            title: Plot title
            color: Optional color for points
            
        Returns:
            plotly.graph_objects.Figure
        """
        fig = go.Figure(data=[go.Scatter3d(
            x=points[:, 0],
            y=points[:, 1],
            z=points[:, 2],
            mode='markers',
            marker=dict(
                size=2,
                color=color or self.colors[0],
                opacity=0.8
            )
        )])
        
        fig.update_layout(
            title=title,
            scene=dict(
                xaxis_title='X (cm)',
                yaxis_title='Y (cm)',
                zaxis_title='Z (cm)',
                aspectmode='data'
            )
        )
        return fig
        
    def plot_multiple_objects(self, objects: Dict[str, np.ndarray],
                            title: str = "Multi-object Reconstruction") -> go.Figure:
        """
        Create interactive 3D plot of multiple point clouds.
        
        Args:
            objects: Dictionary mapping object names to point clouds
            title: Plot title
            
        Returns:
            plotly.graph_objects.Figure
        """
        fig = go.Figure()
        
        for i, (name, points) in enumerate(objects.items()):
            fig.add_trace(go.Scatter3d(
                x=points[:, 0],
                y=points[:, 1],
                z=points[:, 2],
                mode='markers',
                name=name,
                marker=dict(
                    size=2,
                    color=self.colors[i % len(self.colors)],
                    opacity=0.8
                )
            ))
            
        fig.update_layout(
            title=title,
            scene=dict(
                xaxis_title='X (cm)',
                yaxis_title='Y (cm)',
                zaxis_title='Z (cm)',
                aspectmode='data'
            )
        )
        return fig
        
    def plot_volume_estimation(self, points: np.ndarray, 
                             hull_vertices: np.ndarray,
                             volume: float,
                             title: Optional[str] = None) -> go.Figure:
        """
        Create interactive 3D visualization of volume estimation.
        
        Args:
            points: Nx3 array of 3D points
            hull_vertices: Indices of convex hull vertices
            volume: Calculated volume in cubic centimeters
            title: Optional plot title
            
        Returns:
            plotly.graph_objects.Figure
        """
        fig = go.Figure()
        
        # Original points
        fig.add_trace(go.Scatter3d(
            x=points[:, 0],
            y=points[:, 1],
            z=points[:, 2],
            mode='markers',
            name='Points',
            marker=dict(size=2, color='blue', opacity=0.6)
        ))
        
        # Convex hull
        fig.add_trace(go.Mesh3d(
            x=points[hull_vertices, 0],
            y=points[hull_vertices, 1],
            z=points[hull_vertices, 2],
            opacity=0.3,
            color='red',
            name='Convex Hull'
        ))
        
        plot_title = title or f"Volume Estimation: {volume:.2f} cm³"
        fig.update_layout(
            title=plot_title,
            scene=dict(
                xaxis_title='X (cm)',
                yaxis_title='Y (cm)',
                zaxis_title='Z (cm)',
                aspectmode='data'
            )
        )
        return fig
        
    def save_visualization(self, fig: go.Figure,
                         output_dir: Path,
                         filename: str) -> None:
        """
        Save visualization as HTML file.
        
        Args:
            fig: Plotly figure object
            output_dir: Output directory path
            filename: Output filename (should end with .html)
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_path = output_dir / filename
        fig.write_html(str(output_path))
        logger.info(f"Saved visualization to {output_path}")
